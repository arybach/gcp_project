version: '3'
services:

# docker exec -it spark-master /bin/bash
# /spark/bin/pyspark --master --master spark://spark-master:7077
  spark-master:
    # use for standalone spark python app: bde2020/spark-python-template
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8088:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
    command: bash -c "mkdir -p /opt/master && tar -xzf /opt/pyspark_env.tar.gz -C /opt/master && . /opt/master/pyspark_env/bin/activate && /opt/spark/sbin/start-master.sh && spark-submit --archives /opt/pyspark_env.tar.gz#pyspark_env"
    volumes:
      - spark_volume:/spark
      - spark_data:/opt
    networks:
      - zeppelin

  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    command: bash -c "mkdir -p /opt/worker1 && tar -xzf /opt/pyspark_env.tar.gz -C /opt/worker1 && /opt/worker1/pyspark_env/bin/start-worker.sh spark://spark-master:7077 --cores 2 --memory 4g"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_CORES=2"
      - "SPARK_WORKER_MEMORY=4g"
    volumes:
      - spark_data:/opt
    networks:
      - zeppelin

  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    command: bash -c "mkdir -p /opt/worker2 && tar -xzf /opt/pyspark_env.tar.gz -C /opt/worker2 && /opt/worker2/pyspark_env/bin/start-worker.sh spark://spark-master:7077 --cores 2 --memory 4g"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_CORES=2"
      - "SPARK_WORKER_MEMORY=4g"
    volumes:
      - spark_data:/opt
    networks:
      - zeppelin

  zeppelin:
    build: 
      context: .
      dockerfile: Dockerfile
    user: root    
    #image: apache/zeppelin:0.10.1
    container_name: zeppelin
    depends_on:
      - spark-master
    ports:
      - "8084:8080"
    volumes:
      - ./zeppelin/notebook:/opt/zeppelin/notebook
      - ./zeppelin/conf/log4j.properties:/opt/zeppelin/conf/log4j.properties
      - ./zeppelin/conf:/opt/zeppelin/conf # needed for conda env to be activated - manually delete interpreter.json on restarts
      - spark_data:/opt
      - spark_volume:/opt/zeppelin/spark
    environment:
      - "SPARK_HOME=/opt/zeppelin/spark"
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      - zeppelin
    
  # for later - Elastic helps to scale for higher workloads
  # and provides an api with built-in search
  # https://github.com/IBM/elasticsearch-spark-recommender    
  elasticsearch:
    container_name: elasticsearch
    image: bitnami/elasticsearch:latest
    environment:
      - cluster.name=docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
    volumes:
      - ./elasticsearch/data:/usr/share/elasticsearch/data
    networks:
      - zeppelin
    labels:
      container_group: "elasticsearch"    

networks:
  zeppelin:
    driver: bridge

volumes:
    spark_volume:
    spark_data:
