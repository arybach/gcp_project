version: '3'
services:

  hdfs:
    image: bde2020/hadoop-namenode:3.2.1-hadoop2.7.7-java8
    container_name: hadoop-namenode
    ports:
      - "9870:9870"
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - hudi-net

  datanode:
    image: bde2020/hadoop-datanode:3.2.1-hadoop2.7.7-java8
    container_name: hadoop-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
    depends_on:
      - hdfs
    networks:
      - hudi-net

  hudi-master:
    image: apache/hudi
    container_name: hudi-master
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - HADOOP_HOME=/hadoop
    depends_on:
      - spark-master
      - hdfs
    command: bash -c "bin/hudi-cli.sh  --master-spark-url spark://spark-master:7077"
    volumes:
      - ./hudi/data:/app/hudi/data
      - spark_hudi:/spark
      - hudi_data:/opt
    networks:
      - hudi-net

  # set hdfs://<namenode-host>:<namenode-port>/hive/warehouse in hive
  hudi-worker-1:
    image: apache/hudi
    container_name: hudi-worker-1
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_INSTANCES=1
      - HADOOP_HOME=/hadoop
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
      - hdfs
    command: bash -c "bin/start-worker.sh"
    volumes:
      - hudi_data:/opt
    networks:
      - hudi-net

  # set hdfs://<namenode-host>:<namenode-port>/hive/warehouse in hive
  hudi-worker-2:
    image: apache/hudi
    container_name: hudi-worker-2
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_INSTANCES=1
      - HADOOP_HOME=/hadoop
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
      - hdfs
    volumes:
      - hudi_data:/opt
    command: bash -c "bin/start-worker.sh"
    networks:
      - hudi-net

  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2-java11
    container_name: spark-master
    ports:
      - "8088:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - SPARK_ARCHIVE_FILES=/env/pyspark_env.tar.gz#pyspark_env
    #command: bash -c "spark-submit /opt/spark/sbin/start-master.sh"
    volumes:
      - spark_volume:/spark
      - ./env:/env
    networks:
      - zeppelin

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2-java11
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    #command: bash -c "spark-class org.apache.spark.deploy.worker.Worker --cores 2 --memory 4g spark://spark-master:7077"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_CORES=2"
      - "SPARK_WORKER_MEMORY=4g"
      - "SPARK_ARCHIVE_FILES=/env/pyspark_env.tar.gz#pyspark_env"    
    volumes:
      - ./env:/env
    networks:
      - zeppelin

  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2-java11
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    #command: bash -c "spark-class org.apache.spark.deploy.worker.Worker --cores 2 --memory 4g spark://spark-master:7077"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_CORES=2"
      - "SPARK_WORKER_MEMORY=4g"
      - "SPARK_ARCHIVE_FILES=/env/pyspark_env.tar.gz#pyspark_env"    
    volumes:
      - ./env:/env
    networks:
      - zeppelin

  # 0.10.1 is using jdk11 and works with spark up to 3.3.0
  zeppelin:
    image: apache/zeppelin:0.10.1
    user: root
    container_name: zeppelin
    depends_on:
      - spark-master
    ports:
      - "8084:8080"
    volumes:
      - ./zeppelin/notebook:/opt/zeppelin/notebook
      - ./zeppelin/conf/log4j.properties:/opt/zeppelin/conf/log4j.properties
      - ./zeppelin/conf:/opt/zeppelin/conf # needed for conda env to be activated - manually delete interpreter.json on restarts
      - ./env:/env
      - spark_volume:/opt/zeppelin/spark
    environment:
      - "SPARK_HOME=/opt/zeppelin/spark"
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      - hudi-net
    
networks:
  hudi-net:
    driver: bridge

volumes:
    spark_volume:
