version: '3'

services:
  hdfs:
    image: bde2020/hadoop-namenode:3.2.1-hadoop2.7.7-java8
    container_name: hadoop-namenode
    ports:
      - "9870:9870"
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - hudi-net

  datanode:
    image: bde2020/hadoop-datanode:3.2.1-hadoop2.7.7-java8
    container_name: hadoop-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
    depends_on:
      - hdfs
    networks:
      - hudi-net

  hudi-master:
    image: apache/hudi
    container_name: hudi-master
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - HADOOP_HOME=/hadoop
    depends_on:
      - spark-master
      - hdfs
    command: bash -c "bin/hudi-cli.sh  --master-spark-url spark://spark-master:7077"
    volumes:
      - ./hudi/data:/app/hudi/data
      - spark_hudi:/spark
      - hudi_data:/opt
    networks:
      - hudi-net

  # set hdfs://<namenode-host>:<namenode-port>/hive/warehouse in hive
  hudi-worker-1:
    image: apache/hudi
    container_name: hudi-worker-1
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_INSTANCES=1
      - HADOOP_HOME=/hadoop
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
      - hdfs
    command: bash -c "bin/start-worker.sh"
    volumes:
      - hudi_data:/opt
    networks:
      - hudi-net

  # set hdfs://<namenode-host>:<namenode-port>/hive/warehouse in hive
  hudi-worker-2:
    image: apache/hudi
    container_name: hudi-worker-2
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_WORKER_INSTANCES=1
      - HADOOP_HOME=/hadoop
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
      - hdfs
    volumes:
      - hudi_data:/opt
    command: bash -c "bin/start-worker.sh"
    networks:
      - hudi-net

  spark-master:
    image: bde2020/spark-master:3.2.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8088:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - "PYSPARK_PYTHON=python3"
      - "PYSPARK_DRIVER_PYTHON=python3"
    volumes:
      - spark_hudi:/spark
      - hudi_data:/opt
    networks:
      - hudi-net

  zeppelin:
    build:
      context: .
      dockerfile: Dockerfile
    user: root
    container_name: zeppelin
    depends_on:
      - spark-master
      - hdfs
    ports:
      - "8084:8080"
    volumes:
      - ./zeppelin/notebook:/opt/zeppelin/notebook
      - ./zeppelin/conf/log4j.properties:/opt/zeppelin/conf/log4j.properties
      - ./zeppelin/conf:/opt/zeppelin/conf # needed for conda env to be activated - manually delete interpreter.json on restarts
      - hudi_data:/opt
      - spark_hudi:/opt/zeppelin/spark
    environment:
      - "SPARK_HOME=/opt/zeppelin/spark"
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      - hudi-net
     
networks:
  hudi-net:
    driver: bridge

volumes:
    spark_hudi:
    hudi_data: